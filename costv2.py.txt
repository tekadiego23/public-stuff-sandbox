import pandas as pd

# Garde uniquement ce qui sert à l'extraction
base = df_exploded_by_instrument[["job_id", "positions"]].dropna(subset=["positions"])

# positions est un dict: { "sophis/727...": {...}, "sophis/284...": {...} }
# -> on transforme en liste d'items puis explode
meta = (
    base.assign(_items=base["positions"].map(lambda d: list(d.items())))
        .explode("_items", ignore_index=False)
)

# _items = (instrument_id, info_dict)
meta[["instrument_id", "_info"]] = pd.DataFrame(meta["_items"].tolist(), index=meta.index)

# Normalisation du dict info -> colonnes (rapide vs apply)
info_cols = pd.json_normalize(meta["_info"])

# Harmonise les noms (adapte si tes clés exactes diffèrent)
info_cols = info_cols.rename(columns={
    "epi": "epi_list",
    "portfolioId": "portfolioId",
    "legalEntity": "legalEntity",
    "name": "name",
    "isdapPlussType": "isdapPlussType",
    "sophisPayoffCat": "sophisPayoffCat",
    "hmsBook": "hmsBook",
    "desk": "desk",
})

meta = pd.concat([meta[["job_id", "instrument_id"]], info_cols], axis=1)

# (Optionnel) éviter les doublons si même instrument présent plusieurs fois
meta = meta.drop_duplicates(subset=["job_id", "instrument_id"], keep="last")



# Supprime les grosses colonnes qui ne servent plus (positions = énorme)
df_exploded_by_instrument = df_exploded_by_instrument.drop(columns=["positions"], errors="ignore")

df_exploded_by_instrument = df_exploded_by_instrument.merge(
    meta,
    on=["job_id", "instrument_id"],
    how="left"
)


df_exploded_by_instrument = (
    df_exploded_by_instrument
    .explode("epi_list", ignore_index=True)
    .rename(columns={"epi_list": "position_id"})   # si tu veux renommer epi -> position
)


# éviter division par 0
den = df_exploded_by_instrument["sum_instrument_computation_time"].replace({0: pd.NA})

df_exploded_by_instrument["position_cost"] = (
    df_exploded_by_instrument["job_cost"]
    * df_exploded_by_instrument["position_computation_time"]
    / den
).fillna(0.0)





def build_meta_from_positions(df_chunk: pd.DataFrame) -> pd.DataFrame:
    base = df_chunk[["job_id", "positions"]].dropna(subset=["positions"])
    meta_rows = []

    # boucle sur les jobs (mais sur un chunk limité, c'est OK)
    for job_id, pos in zip(base["job_id"].values, base["positions"].values):
        # pos: dict instrument -> info
        for inst, info in pos.items():
            meta_rows.append((job_id, inst, info))

    meta = pd.DataFrame(meta_rows, columns=["job_id", "instrument_id", "_info"])
    info_cols = pd.json_normalize(meta["_info"])
    return pd.concat([meta[["job_id", "instrument_id"]], info_cols], axis=1)

# Exemple de chunking
CHUNK = 200_000   # à ajuster selon RAM
out_meta_parts = []

for start in range(0, len(df_exploded_by_instrument), CHUNK):
    chunk = df_exploded_by_instrument.iloc[start:start+CHUNK]
    out_meta_parts.append(build_meta_from_positions(chunk))

meta = pd.concat(out_meta_parts, ignore_index=True)

